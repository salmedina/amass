{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Sequence as Video\n",
    "AMASS uses [MoSh++](https://amass.is.tue.mpg.de) pipeline to fit [SMPL+H body model](https://mano.is.tue.mpg.de/) to a human motion capture (mocap) session. [These mocaps](https://amass.is.tue.mpg.de/dataset) are from different publicly available datasets. A single data file in amass has the parameters to control gender, pose, shape, global translation and soft tissue dynamics in correspondence with the original motion capture sequence. Here we present code snippets to create a body image with these parameters. Since a mocap is a time sequence you visualize the \"moshed\" per frame results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the environment\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from human_body_prior.tools.omni_tools import copy2cpu as c2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the device to run the body model on.\n",
    "comp_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume you have downloaded the required body model and put that in body_models directory of this repository.\n",
    "For SMPL-H download it from http://mano.is.tue.mpg.de/ and DMPLs you can obtain from http://smpl.is.tue.mpg.de/downloads.\n",
    "If you use any of these models in your research please follow their respective citation rules.\n",
    "One thing to note is that you can obtain basic SMPL+H model from their [website](http://mano.is.tue.mpg.de/), however, this model doesn't have dynamic shape blendshapes, e.g. DMPLs, and has only 10 betas. Doing so will reduce the accuracy of results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "\n",
    "bm_path = '../body_models/smplh/male/model.npz'\n",
    "dmpl_path = '../body_models/dmpls/male/model.npz'\n",
    "\n",
    "num_betas = 10 # number of body parameters\n",
    "num_dmpls = 8 # number of DMPL parameters\n",
    "\n",
    "bm = BodyModel(bm_path=bm_path, num_betas=num_betas, num_dmpls=num_dmpls, path_dmpl=dmpl_path).to(comp_device)\n",
    "faces = c2c(bm.f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data keys available:['trans', 'gender', 'mocap_framerate', 'betas', 'dmpls', 'poses']\n",
      "Vector poses has 156 elements for each of 3600 frames.\n",
      "Vector dmpls has 8 elements for each of 3600 frames.\n",
      "Vector trans has 3 elements for each of 3600 frames.\n",
      "Vector betas has 16 elements constant for the whole sequence.\n",
      "Mocap frequence rate is 120.00.\n",
      "The subject of the mocap sequence is female.\n"
     ]
    }
   ],
   "source": [
    "npz_bdata_path = '/mnt/Alfheim/Data/AMASS/EyesJapanDataset/Eyes_Japan_Dataset/kaiwa/gesture_etc-23-mobile call-kaiwa_poses.npz' # the path to body data\n",
    "bdata = np.load(npz_bdata_path)\n",
    "print('Data keys available:%s'%list(bdata.keys()))\n",
    "print('Vector poses has %d elements for each of %d frames.'%(bdata['poses'].shape[1], bdata['poses'].shape[0]))\n",
    "print('Vector dmpls has %d elements for each of %d frames.'%(bdata['dmpls'].shape[1], bdata['dmpls'].shape[0]))\n",
    "print('Vector trans has %d elements for each of %d frames.'%(bdata['trans'].shape[1], bdata['trans'].shape[0]))\n",
    "print('Vector betas has %d elements constant for the whole sequence.'%bdata['betas'].shape[0])\n",
    "print('Mocap frequence rate is %0.2f.'%bdata['mocap_framerate'])\n",
    "print('The subject of the mocap sequence is %s.'%bdata['gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided sample data also has the original mocap marker data. In the real AMASS dataset, only markers for the test set are included. For the rest of the subsets you can obtain the marker data from their respective websites.\n",
    "In the following we make PyTorch tensors for parameters controlling different part of the body model.\n",
    "\n",
    "**Please note how pose indices for different body parts work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fId = 0\n",
    "root_orient = torch.Tensor(bdata['poses'][fId:fId+1, :3]).to(comp_device) # controls the global root orientation\n",
    "pose_body = torch.Tensor(bdata['poses'][fId:fId+1, 3:66]).to(comp_device) # controls the body\n",
    "pose_hand = torch.Tensor(bdata['poses'][fId:fId+1, 66:]).to(comp_device) # controls the finger articulation\n",
    "betas = torch.Tensor(bdata['betas'][:10][np.newaxis]).to(comp_device) # controls the body shape\n",
    "dmpls = torch.Tensor(bdata['dmpls'][fId:fId+1]).to(comp_device) # controls soft tissue dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required files for viewing out mesh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import trimesh\n",
    "from human_body_prior.tools.omni_tools import colors\n",
    "from human_body_prior.mesh import MeshViewer\n",
    "from human_body_prior.mesh.sphere import points_to_spheres\n",
    "from ipywidgets import interact, widgets\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "def show_image(img_array):\n",
    "    display(Image.fromarray(img_array))\n",
    "\n",
    "imw, imh = 1024, 720\n",
    "mv = MeshViewer(width=imw, height=imh, use_offscreen=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize frame betas and pose_body\n",
    "Let's see how our body looks like using the pose and body shape parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAA7klEQVR4nO2UQWrCUBRFz2+qRYIbCJQQiMFpEdyOe3DiGtyMC3ATIg7EiaCQgG1iK1hp1P9/B+LQvkiCo154s8vhwn3vKbCWCvVUJewf+EBgu53R631UBwzDDVlWDPgsGbpdzWBQR2vN8ThhPH4rl7DftxwO3/h+gOe9iglF4HB4otWKUMrSaLyIQC6nd3t8f23j+GyTRP/pu46YsFark6YJjgNBEIvxxFKM0cznHouFwhhTHrhceuz3X8xmTVYruRQRCDCdulirAUf0Flrs3e6TPP8pYkVJDzaK3nHdJtttSqeTMxqF5YD3qvJv8wternJi12mGdwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=20x20 at 0x7F61BC0499D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fId = 700 # frame id of the mocap sequence\n",
    "\n",
    "root_orient = torch.Tensor(bdata['poses'][fId:fId+1, :3]).to(comp_device) # controls the global root orientation\n",
    "pose_body = torch.Tensor(bdata['poses'][fId:fId+1, 3:66]).to(comp_device) # controls the body\n",
    "betas = torch.Tensor(bdata['betas'][:10][np.newaxis]).to(comp_device) # controls the body shape\n",
    "\n",
    "mv.set_background_color(color=(0., 0., 1.))\n",
    "\n",
    "body = bm(pose_body=pose_body, betas=betas)\n",
    "body_mesh = trimesh.Trimesh(vertices=c2c(body.v[0]), faces=faces, vertex_colors=np.tile(colors['pink'], (6890, 1)))\n",
    "mv.set_static_meshes([body_mesh])\n",
    "body_image = mv.render(render_wireframe=False)\n",
    "show_image(body_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bluescreen2alpha(input_image):\n",
    "    image_copy = np.copy(input_image)\n",
    "    lower_blue = np.array([0, 0, 100, 255])          #[R, G, B]\n",
    "    upper_blue = np.array([120, 100, 255, 255])      #[R, G, B]\n",
    "    alpha_channel = np.invert(cv2.inRange(image_copy, lower_blue, upper_blue))\n",
    "    image_copy[:,:,3] = alpha_channel\n",
    "    return image_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26754d9295ff421c88bdf294644cb32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=600, description='frame_num', max=3599, step=5), Output()), _dom_classes…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.display_body(frame_num)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def display_body(frame_num):\n",
    "    fId = frame_num # frame id of the mocap sequence\n",
    "\n",
    "    root_orient = torch.Tensor(bdata['poses'][fId:fId+1, :3]).to(comp_device) # controls the global root orientation\n",
    "    pose_body = torch.Tensor(bdata['poses'][fId:fId+1, 3:66]).to(comp_device) # controls the body\n",
    "    betas = torch.Tensor(bdata['betas'][:10][np.newaxis]).to(comp_device) # controls the body shape\n",
    "    \n",
    "    mv.set_background_color(color=(0., 0., 1.)) #RGB, blue screen\n",
    "\n",
    "    body = bm(pose_body=pose_body, betas=betas)\n",
    "    body_mesh = trimesh.Trimesh(vertices=c2c(body.v[0]), faces=faces, vertex_colors=np.tile(colors['pink'], (6890, 1)))\n",
    "    mv.set_static_meshes([body_mesh])\n",
    "    body_image = mv.render(render_wireframe=False)\n",
    "    body_image = bluescreen2alpha(body_image)\n",
    "    show_image(body_image)\n",
    "    \n",
    "interact(display_body, frame_num=widgets.IntSlider(min=0, max=bdata['poses'].shape[0]-1, step=5, value=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a male subject sitting and havig the hands open. \n",
    "Let's articulate the fingers as well:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize pose hands\n",
    "To articulate fingers we use the 66:156 pose vector elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20cc81682b5483395aee7b0b4dbe0c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=600, description='frame_num', max=3599, step=5), Output()), _dom_classes…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.display_body_and_hands(frame_num)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def display_body_and_hands(frame_num):\n",
    "    fId = frame_num # frame id of the mocap sequence\n",
    "\n",
    "    root_orient = torch.Tensor(bdata['poses'][fId:fId+1, :3]).to(comp_device) # controls the global root orientation\n",
    "    pose_body = torch.Tensor(bdata['poses'][fId:fId+1, 3:66]).to(comp_device) # controls the body\n",
    "    pose_hand = torch.Tensor(bdata['poses'][fId:fId+1, 66:]).to(comp_device) # controls the finger articulation\n",
    "    betas = torch.Tensor(bdata['betas'][:10][np.newaxis]).to(comp_device) # controls the body shape\n",
    "    dmpls = torch.Tensor(bdata['dmpls'][fId:fId+1]).to(comp_device) # controls soft tissue dynamics\n",
    "\n",
    "    body = bm(pose_body=pose_body, pose_hand=pose_hand, betas=betas)\n",
    "    body_mesh = trimesh.Trimesh(vertices=c2c(body.v[0]), faces=faces, vertex_colors=np.tile(colors['pink'], (6890, 1)))\n",
    "    mv.set_static_meshes([body_mesh])\n",
    "    body_image = mv.render(render_wireframe=False)\n",
    "    show_image(body_image)\n",
    "    \n",
    "interact(display_body_and_hands, frame_num=widgets.IntSlider(min=0, max=bdata['poses'].shape[0]-1, step=5, value=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the subject is holding something with one hand.\n",
    "\n",
    "### Visualize body joints\n",
    "\n",
    "Our body model has also joint locations. You can read their location by accesing `Jtr` attribute of the returned body and visualize them as spheres. Here we render the body transparently to visualize the joints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = bm(pose_body=pose_body, pose_hand = pose_hand, betas=betas)\n",
    "joints = c2c(body.Jtr[0])\n",
    "joints_mesh = points_to_spheres(joints, vc = colors['red'], radius=0.005)\n",
    "mv.set_static_meshes([body_mesh_wfingers] + joints_mesh)\n",
    "body_image_wfingers_joints = mv.render(render_wireframe=True)\n",
    "show_image(body_image_wfingers_joints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize DMPLs\n",
    "\n",
    "You can control the soft tissue dynamics with dmpl parameters. Please have in mind, by nature of dmpl parameters being dynamic, the better appear when animation the whole sequence. Refer to full renders of the parameter sequences in our [website](https://amass.is.tue.mpg.de/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35157bc0d50a4543a578b66948999cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=600, description='frame_num', max=3599, step=5), Output()), _dom_classes…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.display_dmpl(frame_num)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def display_dmpl(frame_num):\n",
    "    fId = frame_num # frame id of the mocap sequence\n",
    "\n",
    "    root_orient = torch.Tensor(bdata['poses'][fId:fId+1, :3]).to(comp_device) # controls the global root orientation\n",
    "    pose_body = torch.Tensor(bdata['poses'][fId:fId+1, 3:66]).to(comp_device) # controls the body\n",
    "    pose_hand = torch.Tensor(bdata['poses'][fId:fId+1, 66:]).to(comp_device) # controls the finger articulation\n",
    "    betas = torch.Tensor(bdata['betas'][:10][np.newaxis]).to(comp_device) # controls the body shape\n",
    "    dmpls = torch.Tensor(bdata['dmpls'][fId:fId+1]).to(comp_device) # controls soft tissue dynamics\n",
    "\n",
    "    body = bm(pose_body=pose_body, pose_hand=pose_hand, betas=betas, dmpls=dmpls)\n",
    "    body_mesh = trimesh.Trimesh(vertices=c2c(body.v[0]), faces=faces, vertex_colors=np.tile(colors['pink'], (6890, 1)))\n",
    "    mv.set_static_meshes([body_mesh])\n",
    "    body_image = mv.render(render_wireframe=False)\n",
    "    show_image(body_image)\n",
    "    \n",
    "interact(display_dmpl, frame_num=widgets.IntSlider(min=0, max=bdata['poses'].shape[0]-1, step=5, value=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the global root orientation\n",
    "\n",
    "In the above examples we dont use the global translation or rotation. To be able to see the subject from the front. However, we can globally control the character position and orientation with trans, and root_orient parameters respectively.\n",
    "\n",
    "Rotation tensor is in radians in the following order: [X, Y, Z]. Where X goes to right, Y is vertical and Z is the line of the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_rotation(frame_num, pitch, yaw, roll):\n",
    "    fId = frame_num # frame id of the mocap sequence\n",
    "\n",
    "    root_orient = torch.Tensor(bdata['poses'][fId:fId+1, :3]).to(comp_device) # controls the global root orientation\n",
    "    pose_body = torch.Tensor(bdata['poses'][fId:fId+1, 3:66]).to(comp_device) # controls the body\n",
    "    pose_hand = torch.Tensor(bdata['poses'][fId:fId+1, 66:]).to(comp_device) # controls the finger articulation\n",
    "    betas = torch.Tensor(bdata['betas'][:10][np.newaxis]).to(comp_device) # controls the body shape\n",
    "    dmpls = torch.Tensor(bdata['dmpls'][fId:fId+1]).to(comp_device) # controls soft tissue dynamics\n",
    "    rotation = torch.tensor([[np.deg2rad(pitch), np.deg2rad(yaw), np.deg2rad(roll)]]).to(comp_device)\n",
    "\n",
    "    body = bm(pose_body=pose_body, pose_hand=pose_hand, betas=betas, dmpls=dmpls, root_orient=rotation)\n",
    "    body_mesh = trimesh.Trimesh(vertices=c2c(body.v[0]), faces=faces, vertex_colors=np.tile(colors['pink'], (6890, 1)))\n",
    "    mv.set_static_meshes([body_mesh])\n",
    "    body_image = mv.render(render_wireframe=False)\n",
    "    show_image(body_image)\n",
    "    \n",
    "interact(display_rotation, \n",
    "         frame_num=widgets.IntSlider(min=0, max=bdata['poses'].shape[0]-1, step=5, value=600),\n",
    "         pitch=widgets.IntSlider(min=-360, max=360, step=5, value=0),\n",
    "         yaw=widgets.IntSlider(min=-360, max=360, step=5, value=0),\n",
    "         roll=widgets.IntSlider(min=-360, max=360, step=5, value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving the camera accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f83d9f3a60494492d77bed8e52b8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=600, description='frame_num', max=3599, step=5), FloatSlider(value=0.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.display_rotation(frame_num, x, y, z, pitch, yaw, roll)>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def display_rotation(frame_num, x, y, z, pitch, yaw, roll):\n",
    "    fId = frame_num # frame id of the mocap sequence\n",
    "\n",
    "    root_orient = torch.Tensor(bdata['poses'][fId:fId+1, :3]).to(comp_device) # controls the global root orientation\n",
    "    pose_body = torch.Tensor(bdata['poses'][fId:fId+1, 3:66]).to(comp_device) # controls the body\n",
    "    pose_hand = torch.Tensor(bdata['poses'][fId:fId+1, 66:]).to(comp_device) # controls the finger articulation\n",
    "    betas = torch.Tensor(bdata['betas'][:10][np.newaxis]).to(comp_device) # controls the body shape\n",
    "    dmpls = torch.Tensor(bdata['dmpls'][fId:fId+1]).to(comp_device) # controls soft tissue dynamics\n",
    "    rotation = torch.tensor([[np.deg2rad(pitch), np.deg2rad(yaw), np.deg2rad(roll)]]).to(comp_device)\n",
    "    \n",
    "    tx, ty, tz = x, y, z\n",
    "    camera_pose = np.array([[1., 0., 0,  tx],  \n",
    "                            [0., 1., 0,  ty],\n",
    "                            [0., 0., 1., tz],\n",
    "                            [0., 0., 0., 1.]])\n",
    "#     camera_pose[:3, 3] = np.array([0, 0, 2.5])\n",
    "    mv.update_camera_pose(camera_pose)\n",
    "    mv.set_background_color([0., 0., 1.])\n",
    "    \n",
    "\n",
    "    body = bm(pose_body=pose_body, pose_hand=pose_hand, betas=betas, dmpls=dmpls, root_orient=rotation)\n",
    "    body_mesh = trimesh.Trimesh(vertices=c2c(body.v[0]), faces=faces, vertex_colors=np.tile(colors['pink'], (6890, 1)))\n",
    "    mv.set_static_meshes([body_mesh])\n",
    "    body_image = mv.render(render_wireframe=False)\n",
    "    body_image = bluescreen2alpha(body_image)\n",
    "    show_image(body_image)\n",
    "    \n",
    "interact(display_rotation, \n",
    "         frame_num=widgets.IntSlider(min=0, max=bdata['poses'].shape[0]-1, step=5, value=600),\n",
    "         x=widgets.FloatSlider(min=-50, max=50, step=0.25, value=0),\n",
    "         y=widgets.FloatSlider(min=-25, max=50, step=0.25, value=0),\n",
    "         z=widgets.FloatSlider(min=0, max=50, step=0.25, value=1),\n",
    "         pitch=widgets.IntSlider(min=-360, max=360, step=10, value=0),\n",
    "         yaw=widgets.IntSlider(min=-360, max=360, step=10, value=0),\n",
    "         roll=widgets.IntSlider(min=-360, max=360, step=10, value=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
